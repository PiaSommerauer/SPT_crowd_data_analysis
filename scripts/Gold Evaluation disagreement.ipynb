{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from clean_annotations import clean_workers\n",
    "from load_data import load_experiment_data, load_expert_data, load_gold_data\n",
    "from aggregation import aggregate_binary_labels\n",
    "\n",
    "from load_data import load_experiment_data\n",
    "from calculate_iaa import get_agreement\n",
    "from utils_analysis import sort_by_key\n",
    "from utils_analysis import load_analysis, load_ct\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support as p_r_f1\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def iaa_dis_agreement(data_dict_list, expert_unit_agreement_dict):\n",
    "    \n",
    "    data_by_agreement = defaultdict(list)\n",
    "    data_by_triple = sort_by_key(data_dict_list, ['relation', 'property', 'concept'])\n",
    "    \n",
    "    for t, gold_expect in expert_unit_agreement_dict.items():\n",
    "        data = data_by_triple[t]\n",
    "        data_by_agreement[gold_expect].extend(data)\n",
    "        \n",
    "    for exp, data in data_by_agreement.items():\n",
    "        agreement = get_agreement(data, v=False)\n",
    "        print(exp, agreement['Krippendorff'])\n",
    "        \n",
    "\n",
    "def get_expert_agreement_labels(expert_annotations):\n",
    "    expert_annotations_by_unit = sort_by_key(expert_annotations, ['relation',\n",
    "                                                              'property', 'concept'])\n",
    "    unit_agreement_dict = dict()\n",
    "    for unit, data in expert_annotations_by_unit.items():\n",
    "        agreements = []\n",
    "        for d in data:\n",
    "            w = d['workerid']\n",
    "            if not w.endswith('_test1'):\n",
    "                for k in d.keys():\n",
    "                    #print(k)\n",
    "                    if k.startswith('disagreement_'):\n",
    "                        agreements.append(k)\n",
    "        n_agreement_annotations = len(agreements)\n",
    "        n_agree = agreements.count('disagreement_agreement')\n",
    "        prop_agreement = n_agree/n_agreement_annotations\n",
    "\n",
    "        if prop_agreement == 1.0:\n",
    "            unit_agreement_dict[unit] = 'agreement'\n",
    "        elif 'disagreement_agreement' in agreements:\n",
    "            unit_agreement_dict[unit] = 'possible_disagreement'\n",
    "        else:\n",
    "            unit_agreement_dict[unit] = 'disagreement'\n",
    "    return unit_agreement_dict\n",
    "\n",
    "\n",
    "def get_agreement_by_unit(data_dict_list):\n",
    "\n",
    "    agreement_unit_dict = dict()\n",
    "    data_by_unit = sort_by_key(data_dict_list, ['relation', 'property', 'concept'])\n",
    "    for unit, dl_unit in data_by_unit.items():\n",
    "        agreement = get_agreement(dl_unit, v=False, disable_kappa=True)\n",
    "        agreement_unit_dict[unit] = agreement['Proportional']\n",
    "    return agreement_unit_dict\n",
    "\n",
    "\n",
    "def get_agreement_by_pair(data_dict_list, ag_metric):\n",
    "\n",
    "    agreement_unit_dict = dict()\n",
    "    data_by_pair = sort_by_key(data_dict_list, ['property', 'concept'])\n",
    "    for pair, dl_unit in data_by_pair.items():\n",
    "        agreement = get_agreement(dl_unit, v=False, disable_kappa=True)\n",
    "        for d in dl_unit:\n",
    "            triple = f\"{d['relation']}-{d['property']}-{d['concept']}\"\n",
    "            agreement_unit_dict[triple] = agreement[ag_metric]\n",
    "    \n",
    "    return agreement_unit_dict\n",
    "\n",
    "def get_contradictions_by_pair(data_dict_list, pair_analysis):\n",
    "    \n",
    "    contradictions_unit_dict = dict()\n",
    "    data_by_pair = sort_by_key(data_dict_list, ['property', 'concept'])\n",
    "    analysis_by_pair = sort_by_key(pair_analysis, ['pair'])\n",
    "    for pair, data in data_by_pair.items():\n",
    "        analysis = analysis_by_pair[pair][0]\n",
    "        n_workers = analysis['n_workers']\n",
    "        n_workers_contradicting = analysis['n_workers_contradicting']\n",
    "        ratio = n_workers_contradicting/n_workers\n",
    "        for d in data:\n",
    "            triple = f\"{d['relation']}-{d['property']}-{d['concept']}\"\n",
    "            contradictions_unit_dict[triple] = ratio\n",
    "    return contradictions_unit_dict\n",
    "            \n",
    "        \n",
    "\n",
    "\n",
    "def get_uqs_by_unit(data_dict_list, ct_units):\n",
    "    ct_by_unit = sort_by_key(ct_units, ['unit'])\n",
    "    uqs_unit_dict = dict()\n",
    "    for d in data_dict_list:\n",
    "        quid = d['quid']\n",
    "        if quid in ct_by_unit:\n",
    "            uqs = ct_by_unit[quid][0]['uqs']\n",
    "            triple = f\"{d['relation']}-{d['property']}-{d['concept']}\"\n",
    "            uqs_unit_dict[triple] = uqs\n",
    "    return uqs_unit_dict\n",
    "\n",
    "\n",
    "def evaluate(expert_unit_agreement_dict, crowd_data, thresh, v=True):\n",
    "    gold = []\n",
    "    predictions = []\n",
    "    correct_predictions = []\n",
    "    for unit, label in expert_unit_agreement_dict.items():\n",
    "        if label == 'disagreement':\n",
    "            label = 'possible_disagreement'\n",
    "        if unit in crowd_data:\n",
    "            score = crowd_data[unit]\n",
    "            if score < thresh:\n",
    "                pred = 'possible_disagreement'\n",
    "            else:\n",
    "                pred = 'agreement'  \n",
    "            if pred == label:\n",
    "                #print(label, pred) \n",
    "                correct_predictions.append(pred)\n",
    "            gold.append(label)\n",
    "            predictions.append(pred)\n",
    "        else:\n",
    "            pass\n",
    "            #print(unit, 'no annotations')\n",
    "    p, r, f1, support = p_r_f1(gold, predictions, average = 'weighted')\n",
    "    if v == True:\n",
    "        print('-------------------------------')\n",
    "        print('\\t gold \\t prediction \\t correct' )\n",
    "        print(\"Agreement\" ,'\\t', gold.count('agreement'),\n",
    "              '\\t', predictions.count('agreement'), '\\t', correct_predictions.count('agreement'))\n",
    "        print(\"Disagreement\",'\\t', gold.count('possible_disagreement'),\n",
    "              '\\t', predictions.count('possible_disagreement'),\n",
    "              '\\t', correct_predictions.count('possible_disagreement'))\n",
    "        print('--------------------------------')\n",
    "        print(p, r, f1)\n",
    "    return f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run4-group_reason_agreement_expert_inspection2/qu40-s_qu40-batch1.csv\n",
      "no summary data\n",
      "run4-group_reason_agreement_False_expert_inspection1/qu30-s_qu30-batch1.csv\n",
      "no summary data\n",
      "run4-group_reason_agreement_expert_inspection3/qu40-s_qu40-batch1.csv\n",
      "no summary data\n",
      "run4-group_reason_agreement_False_expert_inspection2/qu40-s_qu40-batch1.csv\n",
      "no summary data\n",
      "run4-group_reason_agreement_expert_inspection1/qu30-s_qu30-batch1.csv\n",
      "no summary data\n",
      "run4-group_reason_agreement_False_expert_inspection3/qu40-s_qu40-batch1.csv\n",
      "no summary data\n"
     ]
    }
   ],
   "source": [
    "# load expert data \n",
    "\n",
    "run = \"4\"\n",
    "#group1 = 'reason_agreement_expert_inspection1'\n",
    "group = 'reason_agreement*_expert_inspection*'\n",
    "batch = '*'\n",
    "n_q = '*'\n",
    "\n",
    "#run4-group_reason_agreement_expert_inspection1\n",
    "expert_annotations = load_expert_data(run, group, n_q, batch)\n",
    "#expert_annotations2 = load_expert_data(run, group2, n_q, batch)\n",
    "#expert_annotations = expert_annotations1 + expert_annotations2\n",
    "expert_unit_agreement_dict = get_expert_agreement_labels(expert_annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discarded 655.0 annotations.\n"
     ]
    }
   ],
   "source": [
    "run = \"*\"\n",
    "group = 'experiment*'\n",
    "batch = '*'\n",
    "n_q = '*'\n",
    "\n",
    "analysis_type = 'units'\n",
    "ct_units = load_ct(run, group, batch, analysis_type, as_dict=True)\n",
    "\n",
    "analysis_type = 'pairs'\n",
    "pair_analysis =  load_analysis(analysis_type, run, group, batch, as_dict=True)\n",
    "\n",
    "\n",
    "data_dict_list = load_experiment_data(run, group, n_q, batch, remove_not_val = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agreement 0.2674026473334433\n",
      "possible_disagreement 0.04126866987144051\n",
      "disagreement 0.004975124378109541\n"
     ]
    }
   ],
   "source": [
    "# Agreement overview\n",
    "\n",
    "iaa_dis_agreement(data_dict_list, expert_unit_agreement_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------\n",
      "\t gold \t prediction \t correct\n",
      "Agreement \t 40 \t 110 \t 40\n",
      "Disagreement \t 70 \t 0 \t 0\n",
      "--------------------------------\n",
      "0.1322314049586777 0.36363636363636365 0.19393939393939394\n",
      "0.19393939393939394 0.4\n",
      "-------------------------------\n",
      "\t gold \t prediction \t correct\n",
      "Agreement \t 40 \t 92 \t 38\n",
      "Disagreement \t 70 \t 18 \t 16\n",
      "--------------------------------\n",
      "0.7158541941150637 0.4909090909090909 0.44077134986225897\n",
      "0.44077134986225897 0.45\n",
      "-------------------------------\n",
      "\t gold \t prediction \t correct\n",
      "Agreement \t 40 \t 57 \t 25\n",
      "Disagreement \t 70 \t 53 \t 38\n",
      "--------------------------------\n",
      "0.6157503535855074 0.5727272727272728 0.5806417201941465\n",
      "0.5806417201941465 0.5\n",
      "-------------------------------\n",
      "\t gold \t prediction \t correct\n",
      "Agreement \t 40 \t 38 \t 21\n",
      "Disagreement \t 70 \t 72 \t 53\n",
      "--------------------------------\n",
      "0.6693912812333864 0.6727272727272727 0.6708362060474736\n",
      "0.6708362060474736 0.55\n",
      "-------------------------------\n",
      "\t gold \t prediction \t correct\n",
      "Agreement \t 40 \t 29 \t 18\n",
      "Disagreement \t 70 \t 81 \t 59\n",
      "--------------------------------\n",
      "0.6892294593444019 0.7 0.6870141088396199\n",
      "0.6870141088396199 0.6\n",
      "-------------------------------\n",
      "\t gold \t prediction \t correct\n",
      "Agreement \t 40 \t 25 \t 16\n",
      "Disagreement \t 70 \t 85 \t 61\n",
      "--------------------------------\n",
      "0.6894117647058824 0.7 0.6799007444168734\n",
      "0.6799007444168734 0.65\n",
      "-------------------------------\n",
      "\t gold \t prediction \t correct\n",
      "Agreement \t 40 \t 23 \t 15\n",
      "Disagreement \t 70 \t 87 \t 62\n",
      "--------------------------------\n",
      "0.6906546726636682 0.7 0.67576584774037\n",
      "0.67576584774037 0.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/piasommerauer/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data_uqs = get_uqs_by_unit(data_dict_list, ct_units)\n",
    "\n",
    "threshs = [0.4, 0.45, 0.5, 0.55, 0.6, 0.65, 0.7]\n",
    "f1s = []\n",
    "for thresh in threshs:\n",
    "    f1 = evaluate(expert_unit_agreement_dict, data_uqs, thresh, v=True)\n",
    "    f1s.append(f1)\n",
    "    print(f1, thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------\n",
      "\t gold \t prediction \t correct\n",
      "Agreement \t 40 \t 110 \t 40\n",
      "Disagreement \t 70 \t 0 \t 0\n",
      "--------------------------------\n",
      "0.1322314049586777 0.36363636363636365 0.19393939393939394\n",
      "0.19393939393939394 0.4\n",
      "-------------------------------\n",
      "\t gold \t prediction \t correct\n",
      "Agreement \t 40 \t 85 \t 36\n",
      "Disagreement \t 70 \t 25 \t 21\n",
      "--------------------------------\n",
      "0.6885561497326202 0.5181818181818182 0.4907942583732058\n",
      "0.4907942583732058 0.45\n",
      "-------------------------------\n",
      "\t gold \t prediction \t correct\n",
      "Agreement \t 40 \t 56 \t 28\n",
      "Disagreement \t 70 \t 54 \t 42\n",
      "--------------------------------\n",
      "0.6767676767676768 0.6363636363636364 0.6432062561094819\n",
      "0.6432062561094819 0.5\n",
      "-------------------------------\n",
      "\t gold \t prediction \t correct\n",
      "Agreement \t 40 \t 31 \t 19\n",
      "Disagreement \t 70 \t 79 \t 58\n",
      "--------------------------------\n",
      "0.6900775826868111 0.7 0.6900463181775216\n",
      "0.6900463181775216 0.55\n",
      "-------------------------------\n",
      "\t gold \t prediction \t correct\n",
      "Agreement \t 40 \t 25 \t 16\n",
      "Disagreement \t 70 \t 85 \t 61\n",
      "--------------------------------\n",
      "0.6894117647058824 0.7 0.6799007444168734\n",
      "0.6799007444168734 0.6\n",
      "-------------------------------\n",
      "\t gold \t prediction \t correct\n",
      "Agreement \t 40 \t 19 \t 13\n",
      "Disagreement \t 70 \t 91 \t 64\n",
      "--------------------------------\n",
      "0.6963562753036437 0.7 0.6661753868828297\n",
      "0.6661753868828297 0.65\n",
      "-------------------------------\n",
      "\t gold \t prediction \t correct\n",
      "Agreement \t 40 \t 18 \t 12\n",
      "Disagreement \t 70 \t 92 \t 64\n",
      "--------------------------------\n",
      "0.6851119894598154 0.6909090909090909 0.6532760555749062\n",
      "0.6532760555749062 0.7\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data_ag = get_agreement_by_unit(data_dict_list)\n",
    "\n",
    "threshs = [0.4, 0.45, 0.5, 0.55, 0.6, 0.65, 0.7]\n",
    "f1s = []\n",
    "for thresh in threshs:\n",
    "    f1 = evaluate(expert_unit_agreement_dict, data_ag, thresh, v=True)\n",
    "    f1s.append(f1)\n",
    "    print(f1, thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------\n",
      "\t gold \t prediction \t correct\n",
      "Agreement \t 40 \t 70 \t 28\n",
      "Disagreement \t 70 \t 40 \t 28\n",
      "--------------------------------\n",
      "0.5909090909090909 0.509090909090909 0.509090909090909\n",
      "0.509090909090909 0.1\n",
      "\n",
      "-------------------------------\n",
      "\t gold \t prediction \t correct\n",
      "Agreement \t 40 \t 70 \t 28\n",
      "Disagreement \t 70 \t 40 \t 28\n",
      "--------------------------------\n",
      "0.5909090909090909 0.509090909090909 0.509090909090909\n",
      "0.509090909090909 0.15\n",
      "\n",
      "-------------------------------\n",
      "\t gold \t prediction \t correct\n",
      "Agreement \t 40 \t 70 \t 28\n",
      "Disagreement \t 70 \t 40 \t 28\n",
      "--------------------------------\n",
      "0.5909090909090909 0.509090909090909 0.509090909090909\n",
      "0.509090909090909 0.2\n",
      "\n",
      "-------------------------------\n",
      "\t gold \t prediction \t correct\n",
      "Agreement \t 40 \t 60 \t 21\n",
      "Disagreement \t 70 \t 50 \t 31\n",
      "--------------------------------\n",
      "0.5218181818181818 0.4727272727272727 0.4815151515151515\n",
      "0.4815151515151515 0.25\n",
      "\n",
      "-------------------------------\n",
      "\t gold \t prediction \t correct\n",
      "Agreement \t 40 \t 50 \t 13\n",
      "Disagreement \t 70 \t 60 \t 33\n",
      "--------------------------------\n",
      "0.4445454545454545 0.41818181818181815 0.4281274281274282\n",
      "0.4281274281274282 0.3\n",
      "\n",
      "-------------------------------\n",
      "\t gold \t prediction \t correct\n",
      "Agreement \t 40 \t 50 \t 13\n",
      "Disagreement \t 70 \t 60 \t 33\n",
      "--------------------------------\n",
      "0.4445454545454545 0.41818181818181815 0.4281274281274282\n",
      "0.4281274281274282 0.35\n",
      "\n",
      "-------------------------------\n",
      "\t gold \t prediction \t correct\n",
      "Agreement \t 40 \t 50 \t 13\n",
      "Disagreement \t 70 \t 60 \t 33\n",
      "--------------------------------\n",
      "0.4445454545454545 0.41818181818181815 0.4281274281274282\n",
      "0.4281274281274282 0.4\n",
      "\n",
      "-------------------------------\n",
      "\t gold \t prediction \t correct\n",
      "Agreement \t 40 \t 40 \t 13\n",
      "Disagreement \t 70 \t 70 \t 43\n",
      "--------------------------------\n",
      "0.509090909090909 0.509090909090909 0.509090909090909\n",
      "0.509090909090909 0.45\n",
      "\n",
      "-------------------------------\n",
      "\t gold \t prediction \t correct\n",
      "Agreement \t 40 \t 40 \t 13\n",
      "Disagreement \t 70 \t 70 \t 43\n",
      "--------------------------------\n",
      "0.509090909090909 0.509090909090909 0.509090909090909\n",
      "0.509090909090909 0.5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_cont = get_contradictions_by_pair(data_dict_list, pair_analysis)\n",
    "\n",
    "threshs = [0.1, 0.15, 0.20, 0.25, 0.30, 0.35, 0.4, 0.45, 0.5]\n",
    "f1s = []\n",
    "for thresh in threshs:\n",
    "    f1 = evaluate(expert_unit_agreement_dict, data_cont, thresh, v=True)\n",
    "    f1s.append(f1)\n",
    "    print(f1, thresh)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------\n",
      "\t gold \t prediction \t correct\n",
      "Agreement \t 40 \t 110 \t 40\n",
      "Disagreement \t 70 \t 0 \t 0\n",
      "--------------------------------\n",
      "0.1322314049586777 0.36363636363636365 0.19393939393939394\n",
      "0.19393939393939394 0.4\n",
      "\n",
      "-------------------------------\n",
      "\t gold \t prediction \t correct\n",
      "Agreement \t 40 \t 110 \t 40\n",
      "Disagreement \t 70 \t 0 \t 0\n",
      "--------------------------------\n",
      "0.1322314049586777 0.36363636363636365 0.19393939393939394\n",
      "0.19393939393939394 0.45\n",
      "\n",
      "-------------------------------\n",
      "\t gold \t prediction \t correct\n",
      "Agreement \t 40 \t 38 \t 17\n",
      "Disagreement \t 70 \t 72 \t 49\n",
      "--------------------------------\n",
      "0.5957602339181287 0.6 0.5976886962802456\n",
      "0.5976886962802456 0.5\n",
      "\n",
      "-------------------------------\n",
      "\t gold \t prediction \t correct\n",
      "Agreement \t 40 \t 24 \t 15\n",
      "Disagreement \t 70 \t 86 \t 61\n",
      "--------------------------------\n",
      "0.6786469344608879 0.6909090909090909 0.6681235431235432\n",
      "0.6681235431235432 0.55\n",
      "\n",
      "-------------------------------\n",
      "\t gold \t prediction \t correct\n",
      "Agreement \t 40 \t 24 \t 15\n",
      "Disagreement \t 70 \t 86 \t 61\n",
      "--------------------------------\n",
      "0.6786469344608879 0.6909090909090909 0.6681235431235432\n",
      "0.6681235431235432 0.6\n",
      "\n",
      "-------------------------------\n",
      "\t gold \t prediction \t correct\n",
      "Agreement \t 40 \t 24 \t 15\n",
      "Disagreement \t 70 \t 86 \t 61\n",
      "--------------------------------\n",
      "0.6786469344608879 0.6909090909090909 0.6681235431235432\n",
      "0.6681235431235432 0.65\n",
      "\n",
      "-------------------------------\n",
      "\t gold \t prediction \t correct\n",
      "Agreement \t 40 \t 24 \t 15\n",
      "Disagreement \t 70 \t 86 \t 61\n",
      "--------------------------------\n",
      "0.6786469344608879 0.6909090909090909 0.6681235431235432\n",
      "0.6681235431235432 0.7\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ag_metric = 'Proportional'\n",
    "data_ag_pair = get_agreement_by_pair(data_dict_list, ag_metric)\n",
    "\n",
    "#threshs = [0, 0.05, 0.1, 0.15, 0.20]\n",
    "threshs = [0.4, 0.45, 0.5, 0.55, 0.6, 0.65, 0.7]\n",
    "f1s = []\n",
    "for thresh in threshs:\n",
    "    f1 = evaluate(expert_unit_agreement_dict, data_ag_pair, thresh, v=True)\n",
    "    f1s.append(f1)\n",
    "    print(f1, thresh)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
