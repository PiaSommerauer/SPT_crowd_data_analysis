{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from clean_annotations import clean_workers\n",
    "from load_data import load_experiment_data, load_expert_data, load_gold_data\n",
    "from aggregation import aggregate_binary_labels\n",
    "\n",
    "from load_data import load_experiment_data\n",
    "from calculate_iaa import get_agreement\n",
    "from utils_analysis import sort_by_key\n",
    "from utils_analysis import load_analysis, load_ct\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support as p_r_f1\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def iaa_dis_agreement(data_dict_list, expert_unit_agreement_dict):\n",
    "    \n",
    "    data_by_agreement = defaultdict(list)\n",
    "    data_by_triple = sort_by_key(data_dict_list, ['relation', 'property', 'concept'])\n",
    "    \n",
    "    for t, gold_expect in expert_unit_agreement_dict.items():\n",
    "        data = data_by_triple[t]\n",
    "        data_by_agreement[gold_expect].extend(data)\n",
    "        \n",
    "    for exp, data in data_by_agreement.items():\n",
    "        agreement = get_agreement(data, v=False)\n",
    "        print(exp, agreement['Krippendorff'], len(data))\n",
    "        \n",
    "\n",
    "def get_expert_agreement_labels(expert_annotations):\n",
    "    expert_annotations_by_unit = sort_by_key(expert_annotations, ['relation',\n",
    "                                                              'property', 'concept'])\n",
    "    unit_agreement_dict = dict()\n",
    "    for unit, data in expert_annotations_by_unit.items():\n",
    "        agreements = []\n",
    "        for d in data:\n",
    "            w = d['workerid']\n",
    "            if not w.endswith('_test1'):\n",
    "                for k in d.keys():\n",
    "                    #print(k)\n",
    "                    if k.startswith('disagreement_'):\n",
    "                        agreements.append(k)\n",
    "        n_agreement_annotations = len(agreements)\n",
    "        n_agree = agreements.count('disagreement_agreement')\n",
    "        prop_agreement = n_agree/n_agreement_annotations\n",
    "\n",
    "        if prop_agreement == 1.0:\n",
    "            unit_agreement_dict[unit] = 'agreement'\n",
    "        elif 'disagreement_agreement' in agreements:\n",
    "            unit_agreement_dict[unit] = 'possible_disagreement'\n",
    "        else:\n",
    "            unit_agreement_dict[unit] = 'disagreement'\n",
    "    return unit_agreement_dict\n",
    "\n",
    "\n",
    "def get_agreement_by_unit(data_dict_list):\n",
    "\n",
    "    agreement_unit_dict = dict()\n",
    "    data_by_unit = sort_by_key(data_dict_list, ['relation', 'property', 'concept'])\n",
    "    for unit, dl_unit in data_by_unit.items():\n",
    "        agreement = get_agreement(dl_unit, v=False, disable_kappa=True)\n",
    "        agreement_unit_dict[unit] = agreement['Proportional']\n",
    "    return agreement_unit_dict\n",
    "\n",
    "\n",
    "def get_agreement_by_pair(data_dict_list, ag_metric):\n",
    "\n",
    "    agreement_unit_dict = dict()\n",
    "    data_by_pair = sort_by_key(data_dict_list, ['property', 'concept'])\n",
    "    for pair, dl_unit in data_by_pair.items():\n",
    "        agreement = get_agreement(dl_unit, v=False, disable_kappa=True)\n",
    "        for d in dl_unit:\n",
    "            triple = f\"{d['relation']}-{d['property']}-{d['concept']}\"\n",
    "            agreement_unit_dict[triple] = agreement[ag_metric]\n",
    "    \n",
    "    return agreement_unit_dict\n",
    "\n",
    "def get_contradictions_by_pair(data_dict_list, pair_analysis):\n",
    "    \n",
    "    contradictions_unit_dict = dict()\n",
    "    data_by_pair = sort_by_key(data_dict_list, ['property', 'concept'])\n",
    "    analysis_by_pair = sort_by_key(pair_analysis, ['pair'])\n",
    "    for pair, data in data_by_pair.items():\n",
    "        analysis = analysis_by_pair[pair][0]\n",
    "        n_workers = analysis['n_workers']\n",
    "        n_workers_contradicting = analysis['n_workers_contradicting']\n",
    "        ratio = n_workers_contradicting/n_workers\n",
    "        for d in data:\n",
    "            triple = f\"{d['relation']}-{d['property']}-{d['concept']}\"\n",
    "            contradictions_unit_dict[triple] = ratio\n",
    "    return contradictions_unit_dict\n",
    "            \n",
    "        \n",
    "\n",
    "\n",
    "def get_uqs_by_unit(data_dict_list, ct_units):\n",
    "    ct_by_unit = sort_by_key(ct_units, ['unit'])\n",
    "    uqs_unit_dict = dict()\n",
    "    for d in data_dict_list:\n",
    "        quid = d['quid']\n",
    "        if quid in ct_by_unit:\n",
    "            uqs = ct_by_unit[quid][0]['uqs']\n",
    "            triple = f\"{d['relation']}-{d['property']}-{d['concept']}\"\n",
    "            uqs_unit_dict[triple] = uqs\n",
    "    return uqs_unit_dict\n",
    "\n",
    "\n",
    "def evaluate(expert_unit_agreement_dict, crowd_data, thresh, v=True):\n",
    "    gold = []\n",
    "    predictions = []\n",
    "    correct_predictions = []\n",
    "    for unit, label in expert_unit_agreement_dict.items():\n",
    "        if label == 'disagreement':\n",
    "            label = 'possible_disagreement'\n",
    "        if unit in crowd_data:\n",
    "            score = crowd_data[unit]\n",
    "            if score < thresh:\n",
    "                pred = 'possible_disagreement'\n",
    "            else:\n",
    "                pred = 'agreement'  \n",
    "            if pred == label:\n",
    "                #print(label, pred) \n",
    "                correct_predictions.append(pred)\n",
    "            gold.append(label)\n",
    "            predictions.append(pred)\n",
    "        else:\n",
    "            pass\n",
    "            #print(unit, 'no annotations')\n",
    "    p, r, f1, support = p_r_f1(gold, predictions, average = 'weighted')\n",
    "    if v == True:\n",
    "        print('-------------------------------')\n",
    "        print('\\t gold \\t prediction \\t correct' )\n",
    "        print(\"Agreement\" ,'\\t', gold.count('agreement'),\n",
    "              '\\t', predictions.count('agreement'), '\\t', correct_predictions.count('agreement'))\n",
    "        print(\"Disagreement\",'\\t', gold.count('possible_disagreement'),\n",
    "              '\\t', predictions.count('possible_disagreement'),\n",
    "              '\\t', correct_predictions.count('possible_disagreement'))\n",
    "        print('--------------------------------')\n",
    "        print(p, r, f1)\n",
    "    return f1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run4-group_reason_agreement_expert_inspection2/qu40-s_qu40-batch1.csv\n",
      "no summary data\n",
      "run4-group_reason_agreement_False_expert_inspection1/qu30-s_qu30-batch1.csv\n",
      "no summary data\n",
      "run4-group_reason_agreement_expert_inspection4/qu44-s_qu44-batch1.csv\n",
      "no summary data\n",
      "run4-group_reason_agreement_expert_inspection3/qu40-s_qu40-batch1.csv\n",
      "no summary data\n",
      "run4-group_reason_agreement_False_expert_inspection2/qu40-s_qu40-batch1.csv\n",
      "no summary data\n",
      "run4-group_reason_agreement_expert_inspection1/qu30-s_qu30-batch1.csv\n",
      "no summary data\n",
      "run4-group_reason_agreement_False_expert_inspection3/qu40-s_qu40-batch1.csv\n",
      "no summary data\n"
     ]
    }
   ],
   "source": [
    "# load expert data \n",
    "\n",
    "run = \"4\"\n",
    "#group1 = 'reason_agreement_expert_inspection1'\n",
    "group = 'reason_agreement*_expert_inspection*'\n",
    "batch = '*'\n",
    "n_q = '*'\n",
    "\n",
    "#run4-group_reason_agreement_expert_inspection1\n",
    "expert_annotations = load_expert_data(run, group, n_q, batch)\n",
    "#expert_annotations2 = load_expert_data(run, group2, n_q, batch)\n",
    "#expert_annotations = expert_annotations1 + expert_annotations2\n",
    "expert_unit_agreement_dict = get_expert_agreement_labels(expert_annotations)\n",
    "#for k, v in expert_unit_agreement_dict.items():\n",
    " #   print(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discarded 655.0 annotations.\n"
     ]
    }
   ],
   "source": [
    "run = \"*\"\n",
    "group = 'experiment*'\n",
    "batch = '*'\n",
    "n_q = '*'\n",
    "\n",
    "analysis_type = 'units'\n",
    "ct_units = load_ct(run, group, batch, analysis_type, as_dict=True)\n",
    "\n",
    "analysis_type = 'pairs'\n",
    "pair_analysis =  load_analysis(analysis_type, run, group, batch, as_dict=True)\n",
    "\n",
    "\n",
    "data_dict_list = load_experiment_data(run, group, n_q, batch, remove_not_val = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agreement 0.292535628185175 573\n",
      "possible_disagreement 0.10690028757842951 947\n",
      "disagreement 0.23863259210089616 223\n"
     ]
    }
   ],
   "source": [
    "# Agreement overview\n",
    "\n",
    "iaa_dis_agreement(data_dict_list, expert_unit_agreement_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------\n",
      "\t gold \t prediction \t correct\n",
      "Agreement \t 46 \t 154 \t 46\n",
      "Disagreement \t 108 \t 0 \t 0\n",
      "--------------------------------\n",
      "0.08922246584584247 0.2987012987012987 0.1374025974025974\n",
      "0.1374025974025974 0.4\n",
      "-------------------------------\n",
      "\t gold \t prediction \t correct\n",
      "Agreement \t 46 \t 134 \t 44\n",
      "Disagreement \t 108 \t 20 \t 18\n",
      "--------------------------------\n",
      "0.729249854622989 0.4025974025974026 0.3432720057720058\n",
      "0.3432720057720058 0.45\n",
      "-------------------------------\n",
      "\t gold \t prediction \t correct\n",
      "Agreement \t 46 \t 89 \t 31\n",
      "Disagreement \t 108 \t 65 \t 50\n",
      "--------------------------------\n",
      "0.6435025648508794 0.525974025974026 0.5425563090302975\n",
      "0.5425563090302975 0.5\n",
      "-------------------------------\n",
      "\t gold \t prediction \t correct\n",
      "Agreement \t 46 \t 62 \t 25\n",
      "Disagreement \t 108 \t 92 \t 71\n",
      "--------------------------------\n",
      "0.6616637219722773 0.6233766233766234 0.636209716209716\n",
      "0.636209716209716 0.55\n",
      "-------------------------------\n",
      "\t gold \t prediction \t correct\n",
      "Agreement \t 46 \t 53 \t 22\n",
      "Disagreement \t 108 \t 101 \t 77\n",
      "--------------------------------\n",
      "0.6586426836753757 0.6428571428571429 0.6495025442393862\n",
      "0.6495025442393862 0.6\n",
      "-------------------------------\n",
      "\t gold \t prediction \t correct\n",
      "Agreement \t 46 \t 45 \t 20\n",
      "Disagreement \t 108 \t 109 \t 83\n",
      "--------------------------------\n",
      "0.6667725750294557 0.6688311688311688 0.6677746216916723\n",
      "0.6677746216916723 0.65\n",
      "-------------------------------\n",
      "\t gold \t prediction \t correct\n",
      "Agreement \t 46 \t 41 \t 18\n",
      "Disagreement \t 108 \t 113 \t 85\n",
      "--------------------------------\n",
      "0.6586627273007587 0.6688311688311688 0.6630610768541803\n",
      "0.6630610768541803 0.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/piasommerauer/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "data_uqs = get_uqs_by_unit(data_dict_list, ct_units)\n",
    "\n",
    "threshs = [0.4, 0.45, 0.5, 0.55, 0.6, 0.65, 0.7]\n",
    "f1s = []\n",
    "for thresh in threshs:\n",
    "    f1 = evaluate(expert_unit_agreement_dict, data_uqs, thresh, v=True)\n",
    "    f1s.append(f1)\n",
    "    print(f1, thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------\n",
      "\t gold \t prediction \t correct\n",
      "Agreement \t 40 \t 110 \t 40\n",
      "Disagreement \t 70 \t 0 \t 0\n",
      "--------------------------------\n",
      "0.1322314049586777 0.36363636363636365 0.19393939393939394\n",
      "0.19393939393939394 0.4\n",
      "-------------------------------\n",
      "\t gold \t prediction \t correct\n",
      "Agreement \t 40 \t 85 \t 36\n",
      "Disagreement \t 70 \t 25 \t 21\n",
      "--------------------------------\n",
      "0.6885561497326202 0.5181818181818182 0.4907942583732058\n",
      "0.4907942583732058 0.45\n",
      "-------------------------------\n",
      "\t gold \t prediction \t correct\n",
      "Agreement \t 40 \t 56 \t 28\n",
      "Disagreement \t 70 \t 54 \t 42\n",
      "--------------------------------\n",
      "0.6767676767676768 0.6363636363636364 0.6432062561094819\n",
      "0.6432062561094819 0.5\n",
      "-------------------------------\n",
      "\t gold \t prediction \t correct\n",
      "Agreement \t 40 \t 31 \t 19\n",
      "Disagreement \t 70 \t 79 \t 58\n",
      "--------------------------------\n",
      "0.6900775826868111 0.7 0.6900463181775216\n",
      "0.6900463181775216 0.55\n",
      "-------------------------------\n",
      "\t gold \t prediction \t correct\n",
      "Agreement \t 40 \t 25 \t 16\n",
      "Disagreement \t 70 \t 85 \t 61\n",
      "--------------------------------\n",
      "0.6894117647058824 0.7 0.6799007444168734\n",
      "0.6799007444168734 0.6\n",
      "-------------------------------\n",
      "\t gold \t prediction \t correct\n",
      "Agreement \t 40 \t 19 \t 13\n",
      "Disagreement \t 70 \t 91 \t 64\n",
      "--------------------------------\n",
      "0.6963562753036437 0.7 0.6661753868828297\n",
      "0.6661753868828297 0.65\n",
      "-------------------------------\n",
      "\t gold \t prediction \t correct\n",
      "Agreement \t 40 \t 18 \t 12\n",
      "Disagreement \t 70 \t 92 \t 64\n",
      "--------------------------------\n",
      "0.6851119894598154 0.6909090909090909 0.6532760555749062\n",
      "0.6532760555749062 0.7\n"
     ]
    }
   ],
   "source": [
    "data_ag = get_agreement_by_unit(data_dict_list)\n",
    "\n",
    "threshs = [0.4, 0.45, 0.5, 0.55, 0.6, 0.65, 0.7]\n",
    "f1s = []\n",
    "for thresh in threshs:\n",
    "    f1 = evaluate(expert_unit_agreement_dict, data_ag, thresh, v=True)\n",
    "    f1s.append(f1)\n",
    "    print(f1, thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------\n",
      "\t gold \t prediction \t correct\n",
      "Agreement \t 46 \t 154 \t 46\n",
      "Disagreement \t 108 \t 0 \t 0\n",
      "--------------------------------\n",
      "0.08922246584584247 0.2987012987012987 0.1374025974025974\n",
      "0.1374025974025974 0.4\n",
      "\n",
      "-------------------------------\n",
      "\t gold \t prediction \t correct\n",
      "Agreement \t 46 \t 154 \t 46\n",
      "Disagreement \t 108 \t 0 \t 0\n",
      "--------------------------------\n",
      "0.08922246584584247 0.2987012987012987 0.1374025974025974\n",
      "0.1374025974025974 0.45\n",
      "\n",
      "-------------------------------\n",
      "\t gold \t prediction \t correct\n",
      "Agreement \t 46 \t 82 \t 23\n",
      "Disagreement \t 108 \t 72 \t 49\n",
      "--------------------------------\n",
      "0.5610547988596769 0.4675324675324675 0.4891639610389611\n",
      "0.4891639610389611 0.5\n",
      "\n",
      "-------------------------------\n",
      "\t gold \t prediction \t correct\n",
      "Agreement \t 46 \t 58 \t 21\n",
      "Disagreement \t 108 \t 96 \t 71\n",
      "--------------------------------\n",
      "0.6268193013882669 0.5974025974025974 0.6087882705529765\n",
      "0.6087882705529765 0.55\n",
      "\n",
      "-------------------------------\n",
      "\t gold \t prediction \t correct\n",
      "Agreement \t 46 \t 48 \t 20\n",
      "Disagreement \t 108 \t 106 \t 80\n",
      "--------------------------------\n",
      "0.6537409131748755 0.6493506493506493 0.6514424132240796\n",
      "0.6514424132240796 0.6\n",
      "\n",
      "-------------------------------\n",
      "\t gold \t prediction \t correct\n",
      "Agreement \t 46 \t 38 \t 18\n",
      "Disagreement \t 108 \t 116 \t 88\n",
      "--------------------------------\n",
      "0.6735097932920074 0.6883116883116883 0.6790352504638218\n",
      "0.6790352504638218 0.65\n",
      "\n",
      "-------------------------------\n",
      "\t gold \t prediction \t correct\n",
      "Agreement \t 46 \t 38 \t 18\n",
      "Disagreement \t 108 \t 116 \t 88\n",
      "--------------------------------\n",
      "0.6735097932920074 0.6883116883116883 0.6790352504638218\n",
      "0.6790352504638218 0.7\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ag_metric = 'Proportional'\n",
    "data_ag_pair = get_agreement_by_pair(data_dict_list, ag_metric)\n",
    "\n",
    "#threshs = [0, 0.05, 0.1, 0.15, 0.20]\n",
    "threshs = [0.4, 0.45, 0.5, 0.55, 0.6, 0.65, 0.7]\n",
    "f1s = []\n",
    "for thresh in threshs:\n",
    "    f1 = evaluate(expert_unit_agreement_dict, data_ag_pair, thresh, v=True)\n",
    "    f1s.append(f1)\n",
    "    print(f1, thresh)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------\n",
      "\t gold \t prediction \t correct\n",
      "Agreement \t 46 \t 100 \t 31\n",
      "Disagreement \t 108 \t 54 \t 39\n",
      "--------------------------------\n",
      "0.5990909090909091 0.45454545454545453 0.4645080946450809\n",
      "0.4645080946450809 0.1\n",
      "\n",
      "-------------------------------\n",
      "\t gold \t prediction \t correct\n",
      "Agreement \t 46 \t 80 \t 29\n",
      "Disagreement \t 108 \t 74 \t 57\n",
      "--------------------------------\n",
      "0.6484687609687609 0.5584415584415584 0.5767724339152911\n",
      "0.5767724339152911 0.15\n",
      "\n",
      "-------------------------------\n",
      "\t gold \t prediction \t correct\n",
      "Agreement \t 46 \t 80 \t 29\n",
      "Disagreement \t 108 \t 74 \t 57\n",
      "--------------------------------\n",
      "0.6484687609687609 0.5584415584415584 0.5767724339152911\n",
      "0.5767724339152911 0.2\n",
      "\n",
      "-------------------------------\n",
      "\t gold \t prediction \t correct\n",
      "Agreement \t 46 \t 60 \t 21\n",
      "Disagreement \t 108 \t 94 \t 69\n",
      "--------------------------------\n",
      "0.6193285437966288 0.5844155844155844 0.5974583981309183\n",
      "0.5974583981309183 0.25\n",
      "\n",
      "-------------------------------\n",
      "\t gold \t prediction \t correct\n",
      "Agreement \t 46 \t 50 \t 13\n",
      "Disagreement \t 108 \t 104 \t 71\n",
      "--------------------------------\n",
      "0.5564335664335665 0.5454545454545454 0.5506360777587194\n",
      "0.5506360777587194 0.3\n",
      "\n",
      "-------------------------------\n",
      "\t gold \t prediction \t correct\n",
      "Agreement \t 46 \t 50 \t 13\n",
      "Disagreement \t 108 \t 104 \t 71\n",
      "--------------------------------\n",
      "0.5564335664335665 0.5454545454545454 0.5506360777587194\n",
      "0.5506360777587194 0.35\n",
      "\n",
      "-------------------------------\n",
      "\t gold \t prediction \t correct\n",
      "Agreement \t 46 \t 50 \t 13\n",
      "Disagreement \t 108 \t 104 \t 71\n",
      "--------------------------------\n",
      "0.5564335664335665 0.5454545454545454 0.5506360777587194\n",
      "0.5506360777587194 0.4\n",
      "\n",
      "-------------------------------\n",
      "\t gold \t prediction \t correct\n",
      "Agreement \t 46 \t 40 \t 13\n",
      "Disagreement \t 108 \t 114 \t 81\n",
      "--------------------------------\n",
      "0.5953691045796309 0.6103896103896104 0.6020635555519278\n",
      "0.6020635555519278 0.45\n",
      "\n",
      "-------------------------------\n",
      "\t gold \t prediction \t correct\n",
      "Agreement \t 46 \t 40 \t 13\n",
      "Disagreement \t 108 \t 114 \t 81\n",
      "--------------------------------\n",
      "0.5953691045796309 0.6103896103896104 0.6020635555519278\n",
      "0.6020635555519278 0.5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# contradiction counts\n",
    "\n",
    "data_cont = get_contradictions_by_pair(data_dict_list, pair_analysis)\n",
    "\n",
    "threshs = [0.1, 0.15, 0.20, 0.25, 0.30, 0.35, 0.4, 0.45, 0.5]\n",
    "f1s = []\n",
    "for thresh in threshs:\n",
    "    f1 = evaluate(expert_unit_agreement_dict, data_cont, thresh, v=True)\n",
    "    f1s.append(f1)\n",
    "    print(f1, thresh)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
