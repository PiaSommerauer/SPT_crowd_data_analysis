{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "from clean_annotations import clean_workers\n",
    "from load_data import load_experiment_data, load_gold_data\n",
    "from aggregation import aggregate_binary_labels\n",
    "\n",
    "from load_data import load_experiment_data\n",
    "from calculate_iaa import get_agreement\n",
    "from utils_analysis import sort_by_key\n",
    "from utils_analysis import load_analysis, load_ct\n",
    "from utils_analysis import load_contradiction_pairs\n",
    "from utils_analysis import collect_contradictions\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support as p_r_f1\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def iaa_dis_agreement(data_dict_list, expert_unit_agreement_dict):\n",
    "    \n",
    "    data_by_agreement = defaultdict(list)\n",
    "    data_by_triple = sort_by_key(data_dict_list, ['relation', 'property', 'concept'])\n",
    "    \n",
    "    for t, gold_expect in expert_unit_agreement_dict.items():\n",
    "        data = data_by_triple[t]\n",
    "        data_by_agreement[gold_expect].extend(data)\n",
    "    \n",
    "        \n",
    "    for exp, data in data_by_agreement.items():\n",
    "        agreement = get_agreement(data, v=False)\n",
    "        data_by_triple = sort_by_key(data, ['relation', 'property', 'concept'])\n",
    "        print(exp, agreement['Krippendorff'], len(data_by_triple))\n",
    "        \n",
    "\n",
    "def get_expected_behavior(gold):\n",
    "    unit_behavior_dict = dict()\n",
    "    for d in gold:\n",
    "        unit =  f\"{d['relation']}-{d['property']}-{d['concept']}\"\n",
    "        exp = d['expected_agreement']\n",
    "        cnt = d['disagreement_cnt']\n",
    "        if exp == 'disagreement' and cnt >= 3:\n",
    "            exp = 'certain_disagreement'\n",
    "        #if exp != 'agreement':\n",
    "        unit_behavior_dict[unit] = exp\n",
    "        #else:\n",
    "            #unit_behavior_dict[unit] = 'agreement'\n",
    "    return unit_behavior_dict\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_agreement_by_unit(data_dict_list):\n",
    "\n",
    "    agreement_unit_dict = dict()\n",
    "    data_by_unit = sort_by_key(data_dict_list, ['relation', 'property', 'concept'])\n",
    "    for unit, dl_unit in data_by_unit.items():\n",
    "        agreement = get_agreement(dl_unit, v=False, disable_kappa=True)\n",
    "        agreement_unit_dict[unit] = agreement['Proportional']\n",
    "    return agreement_unit_dict\n",
    "\n",
    "\n",
    "def get_agreement_by_pair(data_dict_list, ag_metric):\n",
    "\n",
    "    agreement_unit_dict = dict()\n",
    "    data_by_pair = sort_by_key(data_dict_list, ['property', 'concept'])\n",
    "    for pair, dl_unit in data_by_pair.items():\n",
    "        agreement = get_agreement(dl_unit, v=False, disable_kappa=True)\n",
    "        for d in dl_unit:\n",
    "            triple = f\"{d['relation']}-{d['property']}-{d['concept']}\"\n",
    "            agreement_unit_dict[triple] = agreement[ag_metric]\n",
    "    \n",
    "    return agreement_unit_dict\n",
    "\n",
    "def get_contradictions_by_pair(data_dict_list):\n",
    "    contradictions = load_contradiction_pairs()\n",
    "    contradictions_unit_dict = dict()\n",
    "    data_by_pair = sort_by_key(data_dict_list, ['property', 'concept'])\n",
    "    analysis_by_pair = sort_by_key(pair_analysis, ['pair'])\n",
    "    for pair, data_pair in data_by_pair.items():\n",
    "        data_by_worker = sort_by_key(data_pair, ['workerid'])\n",
    "        n_possible_contradictions = 0\n",
    "        n_contradictions = 0\n",
    "        for w, data in data_by_worker.items():\n",
    "            pair_worker_cont = collect_contradictions(data, contradictions, threshold = 0)\n",
    "            relations = [d['relation'] for d in data]\n",
    "            for r1, r2 in contradictions:\n",
    "                if r1 in relations and r2 in relations:\n",
    "                    n_possible_contradictions += 1\n",
    "            n_contradictions += len(pair_worker_cont)\n",
    "        relations = set([d['relation'] for d in data_pair])\n",
    "        for r in relations:\n",
    "            unit = f'{r}-{pair}'\n",
    "            if n_possible_contradictions == 0:\n",
    "                contradictions_unit_dict[unit] = 0\n",
    "            else:\n",
    "                contradictions_unit_dict[unit] = n_contradictions/n_possible_contradictions\n",
    "\n",
    "    return contradictions_unit_dict\n",
    "            \n",
    "        \n",
    "\n",
    "\n",
    "def get_uqs_by_unit(data_dict_list, ct_units):\n",
    "    ct_by_unit = sort_by_key(ct_units, ['unit'])\n",
    "    uqs_unit_dict = dict()\n",
    "    for d in data_dict_list:\n",
    "        quid = d['quid']\n",
    "        if quid in ct_by_unit:\n",
    "            uqs = ct_by_unit[quid][0]['uqs']\n",
    "            triple = f\"{d['relation']}-{d['property']}-{d['concept']}\"\n",
    "            uqs_unit_dict[triple] = uqs\n",
    "    return uqs_unit_dict\n",
    "\n",
    "\n",
    "def disagreement_acc(target_units_true, target_units_false, unit_score_dict, thresh, below = True):\n",
    "    \n",
    "    predictions = []\n",
    "    labels = [True for u in target_units_true]\n",
    "    [labels.append(False) for u in target_units_false]\n",
    "    target_units = target_units_true + target_units_false\n",
    "    \n",
    "    for u in target_units:\n",
    "        score = unit_score_dict[u]\n",
    "        if below == True:\n",
    "            if score < thresh:\n",
    "                predictions.append(True)\n",
    "            else:\n",
    "                predictions.append(False)\n",
    "        elif below == False:\n",
    "            if score > thresh:\n",
    "                predictions.append(True)\n",
    "            else:\n",
    "                 predictions.append(False)\n",
    "    p, r, f1, support = p_r_f1(labels, predictions, average = 'micro')\n",
    "    print('f1', round(f1, 2))\n",
    "    #print('p', p)\n",
    "    #print('r', r)\n",
    "    correct_pos = []\n",
    "    correct_neg = []\n",
    "    for u, l, pred in zip(target_units, labels, predictions):\n",
    "        if l == pred == True:\n",
    "            correct_pos.append(u)\n",
    "        elif l==pred==False:\n",
    "            correct_neg.append(u)\n",
    "    acc_true = len(correct_pos)/len(target_units_true)\n",
    "    acc_false = len(correct_neg)/len(target_units_false)\n",
    "    acc_total = (len(correct_pos) + len(correct_neg)) / len(target_units)\n",
    "    \n",
    "    print('acc true', round(acc_true, 2))\n",
    "    print('acc neg', round(acc_false, 2))\n",
    "    print('acc total', round(acc_total, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer': 'false', 'completionurl': 'expert_annotation', 'concept': 'shovel', 'disagreement_cnt': 4, 'expected_agreement': 'disagreement', 'property': 'roll', 'quid': 'impossible-shovel-roll', 'relation': 'impossible', 'workerid': 'gold'}\n",
      "number of gold instances:  154\n"
     ]
    }
   ],
   "source": [
    "# load expert data \n",
    "\n",
    "# load gold\n",
    "group = 'reason_agreement*_expert_inspection*'\n",
    "run = 4\n",
    "gold = load_gold_data(run, group)\n",
    "print(gold[0])\n",
    "for d in gold:\n",
    "    if 'answer' not in d:\n",
    "        print(d)\n",
    "print('number of gold instances: ', len(gold))\n",
    "\n",
    "expert_unit_agreement_dict = get_expected_behavior(gold)\n",
    "\n",
    "\n",
    "#for k, v in expert_unit_agreement_dict.items():\n",
    " #   print(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discarded 655.0 annotations.\n"
     ]
    }
   ],
   "source": [
    "run = \"*\"\n",
    "group = 'experiment*'\n",
    "batch = '*'\n",
    "n_q = '*'\n",
    "\n",
    "analysis_type = 'units'\n",
    "ct_units = load_ct(run, group, batch, analysis_type, as_dict=True)\n",
    "\n",
    "analysis_type = 'pairs'\n",
    "pair_analysis =  load_analysis(analysis_type, run, group, batch, as_dict=True)\n",
    "\n",
    "\n",
    "crowd = load_experiment_data(run, group, n_q, batch, remove_not_val = True)\n",
    "\n",
    "run = \"*\"\n",
    "group = 'experiment*'\n",
    "batch = '*'\n",
    "n_q = '*'\n",
    "\n",
    "metric = 'contradictions'\n",
    "unit = 'batch'\n",
    "n_stdv = 0.5\n",
    "crowd_clean = clean_workers(crowd, run, group, batch, metric, unit, n_stdv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "certain_disagreement 0.21325657742636528 41\n",
      "agreement 0.22530689627444067 49\n",
      "possible_disagreement 0.12087774980792576 48\n",
      "disagreement 0.14993472839217536 16\n",
      "\n",
      "certain_disagreement 0.2168361244019137 41\n",
      "agreement 0.27655299539170497 49\n",
      "possible_disagreement 0.1230574217264544 48\n",
      "disagreement 0.11274762248085968 16\n"
     ]
    }
   ],
   "source": [
    "# Agreement overview\n",
    "\n",
    "iaa_dis_agreement(crowd, expert_unit_agreement_dict)\n",
    "print()\n",
    "iaa_dis_agreement(crowd_clean, expert_unit_agreement_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "impossible-roll-shovel\n",
      "unusual-red-carrot\n",
      "implied_category-dangerous-freebooter\n",
      "rare-red-carrot\n",
      "rare-roll-shovel\n",
      "variability_limited-yellow-pineapple\n",
      "variability_limited-square-recliner\n",
      "variability_open-square-recliner\n",
      "creative-square-recliner\n",
      "unusual-square-recliner\n",
      "unusual-yellow-leopard\n",
      "implied_category-round-pen\n",
      "typical_of_concept-round-pen\n",
      "implied_category-square-recliner\n",
      "typical_of_concept-square-recliner\n",
      "rare-square-recliner\n",
      "impossible-yellow-leopard\n",
      "variability_limited-round-pen\n",
      "creative-fly-stock\n",
      "rare-fly-stock\n",
      "unusual-fly-stock\n",
      "impossible-fly-stock\n",
      "implied_category-wheels-cruiser\n",
      "unusual-wheels-cruiser\n",
      "implied_category-fly-stock\n",
      "typical_of_property-fly-stock\n",
      "typical_of_concept-fly-stock\n",
      "afforded_unusual-fly-stock\n",
      "afforded_usual-fly-stock\n",
      "creative-wheels-cruiser\n",
      "variability_open-wheels-cruiser\n",
      "rare-wheels-cruiser\n",
      "impossible-wheels-cruiser\n",
      "rare-black-rhino\n",
      "creative-yellow-leopard\n",
      "impossible-square-recliner\n",
      "unusual-roll-shovel\n",
      "impossible-red-carrot\n",
      "implied_category-fly-arrow\n",
      "affording_activity-yellow-buttercup\n",
      "implied_category-roll-tire\n"
     ]
    }
   ],
   "source": [
    "for u, ag in expert_unit_agreement_dict.items():\n",
    "    if ag == 'certain_disagreement':\n",
    "        print(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41 49\n",
      "n_sd 0\n",
      "f1 0.5\n",
      "acc true 0.68\n",
      "acc neg 0.35\n",
      "acc total 0.5\n",
      "---\n",
      "n_sd 0.5\n",
      "f1 0.51\n",
      "acc true 0.63\n",
      "acc neg 0.41\n",
      "acc total 0.51\n",
      "---\n",
      "n_sd 1\n",
      "f1 0.53\n",
      "acc true 0.44\n",
      "acc neg 0.61\n",
      "acc total 0.53\n",
      "---\n",
      "n_sd 1.5\n",
      "f1 0.54\n",
      "acc true 0.0\n",
      "acc neg 1.0\n",
      "acc total 0.54\n",
      "---\n",
      "n_sd 2\n",
      "f1 0.54\n",
      "acc true 0.0\n",
      "acc neg 1.0\n",
      "acc total 0.54\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "from statistics import stdev\n",
    "\n",
    "# uqs\n",
    "unit_uqs_dict = get_uqs_by_unit(crowd, ct_units)\n",
    "\n",
    "mean = sum(unit_uqs_dict.values())/len(unit_uqs_dict)\n",
    "sd = stdev(unit_uqs_dict.values())\n",
    "\n",
    "\n",
    "# get disagreement \n",
    "target_units_dis = [u for u, ex in expert_unit_agreement_dict.items() if ex == 'certain_disagreement']\n",
    "target_units_ag = [u for u, ex in expert_unit_agreement_dict.items() if ex == 'agreement']\n",
    "\n",
    "print(len(target_units_dis), len(target_units_ag))\n",
    "\n",
    "for n_sd in [0, 0.5, 1, 1.5, 2]:\n",
    "    print('n_sd', n_sd)\n",
    "    thresh = mean - (sd * n_sd)\n",
    "    \n",
    "    disagreement_acc(target_units_dis, target_units_ag, unit_uqs_dict, thresh, below = True)\n",
    "    print('---')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 0.48\n",
      "acc true 0.71\n",
      "acc neg 0.29\n",
      "acc total 0.48\n",
      "---\n",
      "f1 0.52\n",
      "acc true 0.68\n",
      "acc neg 0.39\n",
      "acc total 0.52\n",
      "---\n",
      "f1 0.57\n",
      "acc true 0.46\n",
      "acc neg 0.65\n",
      "acc total 0.57\n",
      "---\n",
      "f1 0.54\n",
      "acc true 0.0\n",
      "acc neg 1.0\n",
      "acc total 0.54\n",
      "---\n",
      "f1 0.54\n",
      "acc true 0.0\n",
      "acc neg 1.0\n",
      "acc total 0.54\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# propertional agreemnt\n",
    "\n",
    "unit_ag_dict = get_agreement_by_unit(crowd)\n",
    "\n",
    "mean = sum(unit_ag_dict.values())/len(unit_ag_dict)\n",
    "sd = stdev(unit_ag_dict.values())\n",
    "target_units_dis = [u for u, ex in expert_unit_agreement_dict.items() if ex == 'certain_disagreement']\n",
    "target_units_ag = [u for u, ex in expert_unit_agreement_dict.items() if ex == 'agreement']\n",
    "\n",
    "\n",
    "for n_sd in [0, 0.5, 1, 1.5, 2]:\n",
    "    \n",
    "    thresh = mean - (sd * n_sd)\n",
    "    \n",
    "    disagreement_acc(target_units_dis, target_units_ag, unit_ag_dict, thresh, below = True)\n",
    "    print('---')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "f1 0.56\n",
      "acc true 0.68\n",
      "acc neg 0.45\n",
      "acc total 0.56\n",
      "---\n",
      "0.5\n",
      "f1 0.59\n",
      "acc true 0.68\n",
      "acc neg 0.51\n",
      "acc total 0.59\n",
      "---\n",
      "1\n",
      "f1 0.58\n",
      "acc true 0.44\n",
      "acc neg 0.69\n",
      "acc total 0.58\n",
      "---\n",
      "1.5\n",
      "f1 0.52\n",
      "acc true 0.05\n",
      "acc neg 0.92\n",
      "acc total 0.52\n",
      "---\n",
      "2\n",
      "f1 0.53\n",
      "acc true 0.0\n",
      "acc neg 0.98\n",
      "acc total 0.53\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# propertional agreemnt clean\n",
    "\n",
    "unit_ag_dict = get_agreement_by_unit(crowd_clean)\n",
    "\n",
    "mean = sum(unit_ag_dict.values())/len(unit_ag_dict)\n",
    "sd = stdev(unit_ag_dict.values())\n",
    "\n",
    "\n",
    "# get disagreement \n",
    "\n",
    "for n_sd in [0, 0.5, 1, 1.5, 2]:\n",
    "    print(n_sd)\n",
    "    thresh = mean - (sd * n_sd)\n",
    "    target_units_dis = [u for u, ex in expert_unit_agreement_dict.items() if ex == 'certain_disagreement']\n",
    "    target_units_ag = [u for u, ex in expert_unit_agreement_dict.items() if ex == 'agreement']\n",
    "    disagreement_acc(target_units_dis, target_units_ag, unit_ag_dict, thresh, below = True)\n",
    "    print('---')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "f1 0.31\n",
      "acc true 0.32\n",
      "acc neg 0.31\n",
      "acc total 0.31\n",
      "---\n",
      "0.5\n",
      "f1 0.32\n",
      "acc true 0.32\n",
      "acc neg 0.33\n",
      "acc total 0.32\n",
      "---\n",
      "1\n",
      "f1 0.41\n",
      "acc true 0.32\n",
      "acc neg 0.49\n",
      "acc total 0.41\n",
      "---\n",
      "1.5\n",
      "f1 0.49\n",
      "acc true 0.29\n",
      "acc neg 0.65\n",
      "acc total 0.49\n",
      "---\n",
      "2\n",
      "f1 0.49\n",
      "acc true 0.29\n",
      "acc neg 0.65\n",
      "acc total 0.49\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "# contradiction counts full\n",
    "\n",
    "unit_cont_dict = get_contradictions_by_pair(crowd)\n",
    "\n",
    "\n",
    "mean = sum(unit_cont_dict.values())/len(unit_cont_dict)\n",
    "sd = stdev(unit_cont_dict.values())\n",
    "\n",
    "target_units_dis = [u for u, ex in expert_unit_agreement_dict.items() if ex == 'certain_disagreement']\n",
    "target_units_ag = [u for u, ex in expert_unit_agreement_dict.items() if ex == 'agreement']\n",
    "\n",
    "# get disagreement \n",
    "\n",
    "for n_sd in [0, 0.5, 1, 1.5, 2]:\n",
    "    print(n_sd)\n",
    "    thresh = mean + (sd * n_sd)\n",
    "    \n",
    "    disagreement_acc(target_units_dis, target_units_ag, unit_cont_dict, thresh, below = False)\n",
    "    print('---')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "f1 0.41\n",
      "acc true 0.32\n",
      "acc neg 0.49\n",
      "acc total 0.41\n",
      "---\n",
      "0.5\n",
      "f1 0.41\n",
      "acc true 0.32\n",
      "acc neg 0.49\n",
      "acc total 0.41\n",
      "---\n",
      "1\n",
      "f1 0.49\n",
      "acc true 0.29\n",
      "acc neg 0.65\n",
      "acc total 0.49\n",
      "---\n",
      "1.5\n",
      "f1 0.49\n",
      "acc true 0.29\n",
      "acc neg 0.65\n",
      "acc total 0.49\n",
      "---\n",
      "2\n",
      "f1 0.41\n",
      "acc true 0.1\n",
      "acc neg 0.67\n",
      "acc total 0.41\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "# contradiction counts full\n",
    "\n",
    "unit_cont_dict = get_contradictions_by_pair(crowd_clean)\n",
    "\n",
    "\n",
    "mean = sum(unit_cont_dict.values())/len(unit_cont_dict)\n",
    "sd = stdev(unit_cont_dict.values())\n",
    "\n",
    "target_units_dis = [u for u, ex in expert_unit_agreement_dict.items() if ex == 'certain_disagreement']\n",
    "target_units_ag = [u for u, ex in expert_unit_agreement_dict.items() if ex == 'agreement']\n",
    "\n",
    "# get disagreement \n",
    "\n",
    "for n_sd in [0, 0.5, 1, 1.5, 2]:\n",
    "    print(n_sd)\n",
    "    thresh = mean + (sd * n_sd)\n",
    "    \n",
    "    disagreement_acc(target_units_dis, target_units_ag, unit_cont_dict, thresh, below = False)\n",
    "    print('---')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
