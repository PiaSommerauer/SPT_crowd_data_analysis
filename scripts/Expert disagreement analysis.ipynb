{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from load_data import load_expert_data\n",
    "from utils_analysis import sort_by_key\n",
    "import pandas as pd\n",
    "import csv\n",
    "from calculate_iaa import get_agreement, get_kappa_pairs, create_matrix\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run4-group_reason_agreement_expert_inspection2/qu40-s_qu40-batch1.csv\n",
      "no summary data\n",
      "run4-group_reason_agreement_False_expert_inspection1/qu30-s_qu30-batch1.csv\n",
      "no summary data\n",
      "run4-group_reason_agreement_expert_inspection4/qu44-s_qu44-batch1.csv\n",
      "no summary data\n",
      "run4-group_reason_agreement_expert_inspection3/qu40-s_qu40-batch1.csv\n",
      "no summary data\n",
      "run4-group_reason_agreement_False_expert_inspection2/qu40-s_qu40-batch1.csv\n",
      "no summary data\n",
      "run4-group_reason_agreement_expert_inspection1/qu30-s_qu30-batch1.csv\n",
      "no summary data\n",
      "run4-group_reason_agreement_False_expert_inspection3/qu40-s_qu40-batch1.csv\n",
      "no summary data\n",
      "41\n",
      "72\n",
      "6\n",
      "35\n",
      "../analyses/expert_annotations/resolved/run4-reason_agreement_False_expert_inspection2.xlsx\n",
      "../analyses/expert_annotations/resolved/run4-reason_agreement_expert_inspection1.xlsx\n",
      "../analyses/expert_annotations/resolved/run4-reason_agreement_False_expert_inspection3.xlsx\n",
      "../analyses/expert_annotations/resolved/run4-reason_agreement_expert_inspection4.xlsx\n",
      "../analyses/expert_annotations/resolved/run4-reason_agreement_expert_inspection3.xlsx\n",
      "../analyses/expert_annotations/resolved/run4-reason_agreement_expert_inspection2.xlsx\n",
      "../analyses/expert_annotations/resolved/run4-reason_agreement_False_expert_inspection1.xlsx\n"
     ]
    }
   ],
   "source": [
    "def get_overview_table(expert_data):\n",
    "    row_dicts = []\n",
    "    data_by_triple = sort_by_key(expert_data, ['relation', 'concept', 'property'])\n",
    "    #all_workers = data_by_worker.keys()\n",
    "    #all_workers = set([d['workerid'] for d in expert_data])\n",
    "    workers_exclude = set(['pia_test1'])\n",
    "    for t, data in data_by_triple.items():\n",
    "        triple_dict = dict()\n",
    "        triple_dict['triple'] = t\n",
    "        triple_dict['description'] = data[0]['description']\n",
    "        for d in data:\n",
    "            w = d['workerid']\n",
    "            if 'answer' in d and w not in workers_exclude:\n",
    "                a = d['answer']\n",
    "                expected_disagreements = []\n",
    "                for k, v in d.items():\n",
    "                    if k.startswith('disagreement_') and v == 'true':\n",
    "                        expected_disagreements.append(k)\n",
    "                triple_dict[f'answer-{w}'] = a\n",
    "                triple_dict[f'expected_behavior-{w}'] = '-'.join(sorted(expected_disagreements))\n",
    "                if 'reason' in d:\n",
    "                    triple_dict[f'reason-{w}'] = d['reason']\n",
    "                if 'comment' in d:\n",
    "                    triple_dict[f'comment-{w}'] = d['comment']\n",
    "        row_dicts.append(triple_dict)\n",
    "    return row_dicts\n",
    "\n",
    "def prepare_discussion(expert_rows):\n",
    "    \n",
    "    agree_all = []\n",
    "    agree_label = []\n",
    "    agree_behavior = []\n",
    "    disagree = []\n",
    "    \n",
    "    new_rows = []\n",
    "    \n",
    "    for r in expert_rows:\n",
    "        answers = [v for k, v in r.items() if k.startswith('answer-') and v != '-']\n",
    "        behav = [v for k, v in r.items() if k.startswith('expected_behavior-') and v != '-']\n",
    "        n_answer_types = len(set(answers))\n",
    "        n_behav_types = len(set(behav))\n",
    "        \n",
    "        if n_answer_types == 1 and n_behav_types == 1:\n",
    "            r['conflict'] = 'none'\n",
    "            agree_all.append(r)\n",
    "        elif n_answer_types == 1 and n_behav_types > 1:\n",
    "            r['conflict'] = 'behavior'\n",
    "            agree_label.append(r)\n",
    "        elif n_behav_types == 1 and n_answer_types > 1:\n",
    "            r['conflict'] = 'label'\n",
    "            agree_behavior.append(r)\n",
    "        else:\n",
    "            disagree.append(r)\n",
    "            r['conflict'] = 'all'\n",
    "    print(len(agree_all))\n",
    "    print(len(agree_label))\n",
    "    print(len(agree_behavior))\n",
    "    print(len(disagree))\n",
    "    \n",
    "    new_rows.extend(disagree)\n",
    "    new_rows.extend(agree_label)\n",
    "    new_rows.extend(agree_behavior)\n",
    "    new_rows.extend(agree_all)\n",
    "    \n",
    "    return new_rows\n",
    "    \n",
    "    \n",
    "    \n",
    "def discussion_to_file(run, n_q, batch, group):\n",
    "    \n",
    "    path = f'../analyses/expert_annotations/discussion/run{run}-{group}.csv'\n",
    "    #reason_agreement_False_expert_inspection1-overview.csv\n",
    "    \n",
    "    expert_data = load_expert_data(run, group, n_q, batch)\n",
    "    expert_rows = get_overview_table(expert_data)\n",
    "    overview_df = pd.DataFrame(expert_rows).fillna(value = '-')\n",
    "    expert_rows_all_colls = overview_df.to_dict(\"records\")\n",
    "    rows_discussion = prepare_discussion(expert_rows_all_colls)\n",
    "    \n",
    "    col_seq = ['conflict', 'triple', 'description']\n",
    "    answers_cols = [k for k in rows_discussion[0].keys() if k.startswith('answer-')]\n",
    "    behav_cols = [k for k in rows_discussion[0].keys() if k.startswith('expected_behavior-')]\n",
    "    reason_cols = [k for k in rows_discussion[0].keys() if k.startswith('reason-')]\n",
    "    comment_cols = [k for k in rows_discussion[0].keys() if k.startswith('comment-')]\n",
    "    col_seq.extend(answers_cols)\n",
    "    col_seq.extend(behav_cols)\n",
    "    col_seq.extend(reason_cols)\n",
    "    col_seq.extend(comment_cols)\n",
    "    df_discussion = pd.DataFrame(rows_discussion)\n",
    "    df_discussion[col_seq].to_csv(path, index=False)\n",
    "    return df_discussion[col_seq]\n",
    "\n",
    "\n",
    "def load_resolved_data(run, group):\n",
    "    \n",
    "    # Gold dicts should have: property, concept, relation, answer, \n",
    "    # expected disagreement: agree, poss_disagree, disagree\n",
    "    expert_dicts = []\n",
    "    \n",
    "    path_dir = f'../analyses/expert_annotations/resolved/'\n",
    "    paths = f'{path_dir}run{run}-{group}.xlsx'\n",
    "    \n",
    "    for path in glob.glob(paths):\n",
    "        print(path)\n",
    "        df = pd.read_excel(path)#, sheetname=f'PageStylerun{run}-{group}')\n",
    "        resolved_dicts = df.to_dict('records')\n",
    "\n",
    "        for d in resolved_dicts:\n",
    "            answers = [v for k, v in d.items() if k.startswith('answer-')\n",
    "                       and v != '-' and not k.endswith('lea')]\n",
    "            workers = [k for k, v in d.items() if k.startswith('answer-')\n",
    "                       and v != '-' and not k.endswith('lea')]\n",
    "            disagreements =  [v.split('-') for k, v in d.items() if k.startswith('expected_behavior-')\n",
    "                             and type(v) == str]\n",
    "\n",
    "            triple = d['triple']\n",
    "            relation, concept, prop = triple.split('-')\n",
    "            for w, a, dis in zip(workers, answers, disagreements):\n",
    "                data_dict = dict()\n",
    "                dis = [d for d in dis if d != '']\n",
    "                if dis == []:\n",
    "                    dis = ['disagreement_agreement']\n",
    "                if dis == ['disagreement_agreement']:\n",
    "                    exp_dis = 'agreement'\n",
    "                elif len(dis) > 1 and 'disagreement_agreement' in dis:\n",
    "                    exp_dis = 'possible_disagreement'\n",
    "                else:\n",
    "                    exp_dis = 'disagreement'\n",
    "                data_dict['relation'] = relation\n",
    "                data_dict['concept'] = concept\n",
    "                data_dict['property'] = prop\n",
    "                data_dict['quid'] = triple\n",
    "                data_dict['workerid'] = w\n",
    "                data_dict['answer'] = a\n",
    "                data_dict['expected_agreement'] = exp_dis \n",
    "                data_dict['completionurl'] = 'expert_annotation'\n",
    "                expert_dicts.append(data_dict)  \n",
    "    return expert_dicts\n",
    "\n",
    "\n",
    "def resolved_to_gold(expert_data_resolved):\n",
    "    \n",
    "    gold_data = []\n",
    "    \n",
    "    data_by_triple = sort_by_key(expert_data_resolved, ['quid'])\n",
    "    for t, data in data_by_triple.items():\n",
    "        gold_dict = data[0]\n",
    "        answers = [str(d['answer']).lower() for d in data]\n",
    "        true_cnt = answers.count('true')\n",
    "        true_prop = true_cnt/len(answers)\n",
    "        if true_prop > 0.5:\n",
    "            a = 'true'\n",
    "        else:\n",
    "            a = 'false'\n",
    "        agreements = [d['expected_agreement'] for d in data]\n",
    "        if len(set(agreements)) == 1:\n",
    "            #print(agreements)\n",
    "            agreement = agreements[0]\n",
    "        elif 'disagreement' in agreements:\n",
    "            agreement = 'possible_disagreement'\n",
    "        gold_dict['answer'] = a\n",
    "        gold_dict['workerid'] = 'gold'\n",
    "        gold_dict['expected_agreement'] = agreement\n",
    "        gold_data.append(gold_dict)\n",
    "    return gold_data\n",
    "        \n",
    "\n",
    "\n",
    "run = 4\n",
    "n_q = '*'\n",
    "batch = '1'\n",
    "group = 'reason_agreement*_expert_inspection*'\n",
    "df = discussion_to_file(run, n_q, batch, group)\n",
    "\n",
    "#overview_df\n",
    "#name = f'run{run}-group_{group}-batch{batch}.csv'.replace('*', '-all-')\n",
    "\n",
    "\n",
    "\n",
    "expert_data_resolved =  load_resolved_data(run, group)\n",
    "gold_data = resolved_to_gold(expert_data_resolved)\n",
    "\n",
    "name = f'run{run}-{group}.csv'.replace('*', '-all-')\n",
    "gold_path = f'../gold_labels/gold_files/{name}'\n",
    "gold_df = pd.DataFrame(gold_data)\n",
    "gold_df.to_csv(gold_path, index=False)\n",
    "\n",
    "#df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "154\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expert IAA (before discussion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run4-group_reason_agreement_expert_inspection2/qu40-s_qu40-batch1.csv\n",
      "no summary data\n",
      "run4-group_reason_agreement_False_expert_inspection1/qu30-s_qu30-batch1.csv\n",
      "no summary data\n",
      "run4-group_reason_agreement_expert_inspection4/qu44-s_qu44-batch1.csv\n",
      "no summary data\n",
      "run4-group_reason_agreement_expert_inspection3/qu40-s_qu40-batch1.csv\n",
      "no summary data\n",
      "run4-group_reason_agreement_False_expert_inspection2/qu40-s_qu40-batch1.csv\n",
      "no summary data\n",
      "run4-group_reason_agreement_expert_inspection1/qu30-s_qu30-batch1.csv\n",
      "no summary data\n",
      "run4-group_reason_agreement_False_expert_inspection3/qu40-s_qu40-batch1.csv\n",
      "no summary data\n",
      "479 477\n",
      "Krippendorff's alpha: 0.5255275697753574\n",
      "Average Cohen's Kappa (pairwise): -\n",
      "Proportional agreement (pairwise): 0.8300865800865797\n",
      "\n"
     ]
    }
   ],
   "source": [
    "run = 4\n",
    "n_q = '*'\n",
    "batch = '1'\n",
    "group = 'reason_agreement*_expert_inspection*'\n",
    "\n",
    "expert_data = load_expert_data(run, group, n_q, batch)\n",
    "\n",
    "expert_data_answer = [d for d in expert_data if 'answer' in d]\n",
    "print(len(expert_data), len(expert_data_answer))\n",
    "ag = get_agreement(expert_data_answer)\n",
    "\n",
    "matrix = create_matrix(expert_data_answer)\n",
    "pair_kappa_dict = get_kappa_pairs(matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IAA after discussion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../analyses/expert_annotations/resolved/run4-reason_agreement_False_expert_inspection2.xlsx\n",
      "../analyses/expert_annotations/resolved/run4-reason_agreement_expert_inspection1.xlsx\n",
      "../analyses/expert_annotations/resolved/run4-reason_agreement_False_expert_inspection3.xlsx\n",
      "../analyses/expert_annotations/resolved/run4-reason_agreement_expert_inspection4.xlsx\n",
      "../analyses/expert_annotations/resolved/run4-reason_agreement_expert_inspection3.xlsx\n",
      "../analyses/expert_annotations/resolved/run4-reason_agreement_expert_inspection2.xlsx\n",
      "../analyses/expert_annotations/resolved/run4-reason_agreement_False_expert_inspection1.xlsx\n",
      "Krippendorff's alpha: 0.6914766480233281\n",
      "Average Cohen's Kappa (pairwise): 0.7178718774974399\n",
      "Proportional agreement (pairwise): 0.9025974025974027\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Krippendorff': 0.6914766480233281,\n",
       " 'Proportional': 0.9025974025974027,\n",
       " 'Av_Cohens_kappa': 0.7178718774974399}"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expert_data_resolved = load_resolved_data(run, group)\n",
    "get_agreement(expert_data_resolved)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['filename', 'listnumber', 'assignmentid', 'hitid', 'workerid', 'origin', 'timestamp', 'partid', 'questionid', 'quid', 'description', 'exampletrue', 'examplefalse', 'run', 'sublist', 'completionurl', 'name', 'id', 'uuid', 'time_taken_batch', 'answer', 'disagreement_agreement', 'relation', 'property', 'concept'])\n",
      "Krippendorff's alpha: 0.09019717089175716\n",
      "Average Cohen's Kappa (pairwise): -\n",
      "Proportional agreement (pairwise): 0.6515151515151512\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# IAA on behavior fine-grained\n",
    "\n",
    "print(expert_data[0].keys())\n",
    "\n",
    "# change 'answer' to behavior\n",
    "\n",
    "for d in expert_data:\n",
    "    if 'disagreement_agreement' in d:\n",
    "        d['answer'] = d['disagreement_agreement']\n",
    "    \n",
    "ag = get_agreement(expert_data_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['relation', 'concept', 'property', 'quid', 'workerid', 'answer', 'expected_agreement', 'completionurl'])\n",
      "Krippendorff's alpha: 0.19219945807005645\n",
      "Average Cohen's Kappa (pairwise): 0.2353137294910123\n",
      "Proportional agreement (pairwise): 0.5660173160173163\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# iaa behavior coarse-grained\n",
    "\n",
    "print(expert_data_resolved[0].keys())\n",
    "\n",
    "for d in expert_data_resolved:\n",
    "    if 'expected_agreement' in d:\n",
    "        d['answer'] = d['expected_agreement']\n",
    "        \n",
    "ag = get_agreement(expert_data_resolved)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
