{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discarded 655.0 annotations.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import crowdtruth\n",
    "from crowdtruth.configuration import DefaultConfig\n",
    "import os\n",
    "\n",
    "\n",
    "def create_time_series(question_dicts, day):\n",
    "    \"\"\"\n",
    "    Create a time series from all the questions answered by a single worker.\n",
    "\n",
    "    :param list question_dicts: List of dicts with question info\n",
    "    :param str day: date information\n",
    "    :return: tuples (containing time and position in original list)\n",
    "    \"\"\"\n",
    "    # create_time_serious of question_dicts\n",
    "    time_tuples = []\n",
    "    #print('number of questions:', len(question_dicts))\n",
    "    for n, qu in enumerate(question_dicts):\n",
    "        time = qu['timestamp']\n",
    "        d, time = time.split(' ')\n",
    "        time = datetime.time.fromisoformat(time)\n",
    "        dt = datetime.datetime.combine(day, time)\n",
    "        time_tuple = (dt, n)\n",
    "        time_tuples.append(time_tuple)\n",
    "    return time_tuples\n",
    "\n",
    "\n",
    "def add_start_end_times(time_tuples, question_dicts, start_dt_a):\n",
    "    \"\"\"\n",
    "    Add start and end times to time series.\n",
    "\n",
    "    :param list time tuples: List of tuples with submission time\n",
    "    and original position in question_dicts (list)\n",
    "    :param list question_dicts: list of all questions answered by one participant\n",
    "    :param datetime.datetime: starting time (Amsterdam timezone)\n",
    "    :return: list of all question dicts including time stat and end time\n",
    "    \"\"\"\n",
    "    all_questions = []\n",
    "    times = sorted(time_tuples)\n",
    "    for n, time_i in enumerate(times):\n",
    "        dt, i = time_i\n",
    "        qu = question_dicts[i]\n",
    "        if n == 0:\n",
    "            time_start = start_dt_a\n",
    "        else:\n",
    "            time_start = time_tuples[n-1][0]\n",
    "        time_finish = dt\n",
    "        qu['_started_at'] = str(time_start)\n",
    "        qu['_created_at'] = str(time_finish)\n",
    "        all_questions.append(qu)\n",
    "    return all_questions\n",
    "\n",
    "def add_time_info(question_dicts, start_dicts=None):\n",
    "    \"\"\"\n",
    "    Add start and end times to time series.\n",
    "\n",
    "    :param list time tuples: List of tuples with submission time\n",
    "    and original position in question_dicts (list)\n",
    "    :param list question_dicts: list of all questions answered by one participant\n",
    "    :return: list of all question dicts including time stat and end time\n",
    "    \"\"\"\n",
    "    # level of a single participant within a batch\n",
    "    # We have start and end time of the participant working on a batch\n",
    "    if start_dicts != None:\n",
    "        start = start_dicts[0]['started_datetime']\n",
    "    else:\n",
    "        start = '2010-01-01 00:00:00.000000'\n",
    "    if type(start) == str:\n",
    "        start_dt = datetime.datetime.fromisoformat(start).replace(tzinfo=pytz.timezone('Europe/London'))\n",
    "        day = start_dt.date()\n",
    "        start_dt_a = start_dt.astimezone(pytz.timezone('Europe/Amsterdam')).replace(tzinfo=None).replace(microsecond=0)\n",
    "        time_tuples = create_time_series(question_dicts, day)\n",
    "        all_questions = add_start_end_times(time_tuples, question_dicts, start_dt_a)\n",
    "    else:\n",
    "        # not encountered so far\n",
    "        print('start not string')\n",
    "        all_questions = []\n",
    "    return all_questions\n",
    "\n",
    "\n",
    "\n",
    "def create_input_df(all_question_dicts):\n",
    "    \"\"\"\n",
    "    Create final dataframe in format expected by crowdtruth (figure8)\n",
    "\n",
    "    :param list all_question_dicts: List of all annotated units with time info\n",
    "    :return: dataframe in expected format\n",
    "    \"\"\"\n",
    "    final_df = pd.DataFrame.from_records(all_question_dicts)\n",
    "    # make name changes so the format will be recognized\n",
    "    final_df.rename(columns={'quid': '_unit_id'}, inplace = True)\n",
    "    final_df.rename(columns={'id': '_id'}, inplace = True)\n",
    "    final_df.rename(columns={'workerid': '_worker_id'}, inplace = True)\n",
    "    final_df.drop(columns = ['assignmentid',\n",
    "    'completionurl', 'exampletrue', 'examplefalse', 'filename', 'hitid',\n",
    "    'listnumber', 'origin', 'partid', 'questionid', 'run', 'sublist', 'timestamp'],\n",
    "    inplace = True)\n",
    "    checks = ['check1', 'check2', 'check3', 'check4']\n",
    "    #for ch in checks:\n",
    "    #    final_df.drop(final_df[final_df['_unit_id'] == ch].index, inplace = True)\n",
    "    # get colums in correct order\n",
    "    col_order = ['_unit_id', '_id', '_worker_id', '_started_at',\\\n",
    "                 '_created_at', 'description', 'relation', 'concept',\n",
    "                 'property', 'answer',]\n",
    "    final_df = final_df[col_order]\n",
    "    return final_df\n",
    "\n",
    "class TestConfig(DefaultConfig):\n",
    "    inputColums = ['description', 'relation', 'concept', 'property']\n",
    "    outputColumns = ['answer']\n",
    "    annotation_separator = ','\n",
    "    open_ended_task = False\n",
    "    annotation_vector = ['true', 'false']\n",
    "\n",
    "    def processJudgments(self, judgments):\n",
    "        # pre-process output to match the values in annotation_vector\n",
    "        for col in self.outputColumns:\n",
    "            # transform to lowercase\n",
    "            judgments[col] = judgments[col].apply(lambda x: str(x).lower())\n",
    "        return judgments\n",
    "\n",
    "def split_unit_annotation_score(unit_scores_df):\n",
    "\n",
    "    col = unit_scores_df['unit_annotation_score']\n",
    "    scores_true = []\n",
    "    scores_false = []\n",
    "\n",
    "    for ind, sc in col.items():\n",
    "        scores_true.append(sc['true'])\n",
    "        scores_false.append(sc['false'])\n",
    "    unit_scores_df['unit_annotation_score_true'] = scores_true\n",
    "    unit_scores_df['unit_annotation_score_false'] = scores_false\n",
    "    \n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    run = '*'\n",
    "    batch = '*'\n",
    "    n_q = '*'\n",
    "    group = 'experiment*'\n",
    "    \n",
    "    name = f'run{run}-group_{group}-batch{batch}'.replace('*', '-all-')\n",
    "\n",
    "    data_dict_list = load_experiment_data(run, group, n_q, batch, remove_not_val = True)\n",
    "    data_dicts_time = add_time_info(data_dict_list)\n",
    "    input_df = create_input_df(data_dicts_time)\n",
    "    input_dir = '../analyses/crowdtruth/input/'\n",
    "    input_path = f'{input_dir}{name}.csv'\n",
    "    os.makedirs(input_dir, exist_ok=True)\n",
    "    input_df.to_csv(input_path, index = False)\n",
    "    \n",
    "    res_dir = '../analyses/crowdtruth/results/'\n",
    "    res_path = f'{res_dir}{name}'\n",
    "    os.makedirs(res_dir, exist_ok=True)\n",
    "\n",
    "    \n",
    "    input_file = input_path\n",
    "    data, config = crowdtruth.load(\n",
    "        file = input_file,\n",
    "        config = TestConfig()\n",
    "    )\n",
    "    results = crowdtruth.run(data, config)\n",
    "\n",
    "    unit_scores = results['units']\n",
    "    split_unit_annotation_score(unit_scores)\n",
    "    unit_scores.to_csv(f'{res_path}-units.csv')\n",
    "\n",
    "    worker_scores = results['workers']\n",
    "    worker_scores.to_csv(f'{res_path}-workers.csv')\n",
    "\n",
    "    annotation_scores = results[\"annotations\"]\n",
    "    annotation_scores.to_csv(f'{res_path}-annotations.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
